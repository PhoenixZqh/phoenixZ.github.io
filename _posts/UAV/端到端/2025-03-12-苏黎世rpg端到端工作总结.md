---
layout:     post


title:      苏黎世端到端工作总结


date:       2025-03-12


author:     phoenixZ


header-img: img/dji3.jpg


catalog: true


tags:
    - rpg

    - 端到端
---


## 🚀DroNet: Learning to Fly by Driving

**核心问题：** 如何让无人机仅依靠前视摄像头在城市环境中安全、平稳地飞行，同时保证计算效率以适应嵌入式系统。
![]({{ site.baseurl }}/img/dronet.png)

### 网络架构

**输入：**  
单张灰度图像，尺寸为 200×200 像素，由无人机的前视摄像头实时捕获。选择灰度图是为了降低计算复杂度，同时保留足够的视觉信息。

**模型结构：**  
基于 8 层残差网络（ResNet-8），包含卷积层、池化层和跳跃连接（skip connections）。设计灵感来源于 ResNet，但大幅精简以适应实时性和嵌入式硬件（如 NVIDIA Jetson TX1）。参数量较小，仅约 310K 个参数，比当时的最佳模型（如 VGG）小 80 倍。

**输出：**  
DroNet 同时预测两个值：
- **转向角度（Steering Angle）：** 一个连续值，表示无人机应向左或向右调整方向的幅度，用于保持路径跟随和避开障碍物。
- **碰撞概率（Collision Probability）：** 一个介于 0 到 1 的值，表示当前路径上存在障碍物的可能性，用于触发减速或悬停等安全行为。

**双输出设计：**  
网络的最后一层分为两个分支：
- 一个回归分支预测转向角度（均方误差损失）。
- 一个分类分支预测碰撞概率（交叉熵损失）。  
联合训练两个任务，确保模型同时关注路径规划和安全。

### 训练数据

**创新点：**  
直接在无人机上收集飞行数据既危险又昂贵，因此 DroNet 使用地面车辆数据训练：
- **转向角度数据：** 来自 Udacity 自动驾驶数据集，包含汽车在城市街道上的驾驶视频和对应的方向盘角度。
- **碰撞概率数据：** 通过骑自行车在城市中收集的自定义数据集，标注前方是否有障碍物（如行人、车辆）。

**数据处理：**  
Udacity 数据提供图像-转向角度对，用于回归任务。自行车数据通过手动标注（障碍物存在与否）生成二分类标签。数据经过预处理（如灰度化、resize）以匹配网络输入。

**迁移假设：**  
地面车辆的视角（低视角）和无人机的高空视角差异较大，但论文假设导航的基本规则（如避开障碍物、跟随路径）在两种场景中是相似的。这种跨领域迁移是 DroNet 的核心创新。

### 导航策略

**控制逻辑：**  
- **转向：** 根据网络预测的转向角度调整无人机的偏航（yaw），保持路径或避障。
- **速度控制：** 基于碰撞概率：
  - 如果概率低于阈值（如 0.5），保持正常速度。
  - 如果概率高于阈值，减速或悬停。
- **高度：** 论文中未明确控制高度，通常假设固定高度飞行。

**实时性：**  
DroNet 在 NVIDIA Jetson TX1 上以 20 FPS 运行，延迟低，适合嵌入式应用。

## 🚀敏捷飞行：Learning High-Speed Flight in the Wild

> 解决问题：
> 1. 传统感知->建图/状态估计->控制 的方法模块化导致通信延迟、误差累积、不适应高速飞行场景
> 2. 通过端到端的思路，增强了系统对噪声的鲁棒性，降低延迟
>
> ⭐️ 可用于智能体自主探索、配送等场景


![敏捷飞行]({{ site.baseurl }}/img/agile_auto.png)


### 输入数据
1. 深度图像： d ∈ R^(640×480)，黑白图，表示障碍物的远近。
2. 状态信息： 速度  $$\mathbf{v} \in \mathbb{R}^3$$ （三维速度向量）姿态 $$ q \in \mathbb{R}^9 $$ （旋转矩阵）。
3. 目标方向： $$\omega \in \mathbb{R}^3$$，一个归一化的向量，指向未来1秒的参考点（从参考轨迹来的）。


### 特权学习：老师生成示范路径

1. 老师利用仿真环境中3D点云数据和地图信息离线生成路径分布
2. 打分细节

  - 成本函数：  c(τ) = ∫₀¹ (λc C_{collision}(τ(t)) + (τ(t) - τ_{ref}(t))ᵀ Q (τ(t) - τ_{ref}(t))) dt
  - \( C_{collision} \): 当离障碍物 < 0.4米时，成本上升（二次函数）
  - \([\tau - \tau_{ref}]\): 偏离参考轨迹的惩罚。
  
3. 采样
   - 用 M-H 算法试5万条路径，成本低的概率高（  $$ P(\tau) = \frac{1}{Z} \exp(-c(\tau)) $$  ）
   - 输出一堆路径分布，不是单条“最好”，而是多种可能。

### 特权学习：学生模仿老师
1. Backbone
使用 MobileNet-V3 从深度图提取特征（高效处理图像）。

2. 输入
- 深度图
- 速度
- 姿态
- 目标方向

3. 输出
  3条路径 $\tau_{n,k} \in \mathbb{R}^{10 \times 3}$ 和它们的碰撞成本。

**总结：用“赢家通吃的原则”，让预测贴近老师的最佳路径**


### 真实飞行
1. 利用训练好的权重，在真实环境中利用退避视界的原则，每秒24次预测路径
2. 过程
   - 双目相机生成深度图
   - 网络预测3条路径+成本
   - 挑成本最低的（若成本接近，选省力的）
   - 模型预测控制器（MPC）执行


## 🚀像素到控制: Demonstrating Agile Flight from Pixels without State Estimation

![像素到控制]({{ site.baseurl }}/img/像素坐标端到端.png)

> 解决问题： 省去中间状态估计再规划的步骤，提速并且可以在硬件上更简单 

### 仿真训练

1. 仿真环境生成数据
- **描述**: 仿真环境生成门框掩码和无人机状态信息。掩码通过透视投影生成（84×84像素），计算效率高（100微秒/帧），噪声少；状态信息是20维向量，仅训练时用。
- **公式**:  
  - 掩码生成无显式公式，基于透视投影：门框边缘离散为5点，投影到图像平面。
  - 状态向量： $s = [p, \tilde{R}, v, \omega, i, d]$
    - $p \in \mathbb{R}^3$: 位置
    - $\tilde{R} \in \mathbb{R}^6$: 姿态（旋转矩阵前两列）
    - $v \in \mathbb{R}^3$: 线速度
    - $\omega \in \mathbb{R}^3$: 角速度
    - $i \in \mathbb{R}^2$: 门框索引
    - $d \in \mathbb{R}^3$: 下一门框相对位置
2. 特征提取
- **描述**: 三层卷积神经网络（CNN）将门框掩码转化为低维特征向量，作为演员和评论家的共享输入。
- **公式**:  
  - $f_t = \text{CNN}(o_t)$
    - $o_t$: 当前掩码（84×84）
    - $f_t$: 特征向量（维度未明确，假设为低维）


3. 演员网络
- **描述**: 输入CNN特征向量和前三步动作，输出当前动作（4维：推力+角速度）。
- **公式**:  
  - 输入： $[f_t, a_{t-1}, a_{t-2}, a_{t-3}]$
    - $a_{t-i}$: 前第$i$步动作， $a_{t-i} = [c_{t-i}, \omega_{B,\text{ref}, t-i}]$
    - $c$: 质量归一化总推力
    - $\omega_{B,\text{ref}} \in \mathbb{R}^3$: 角速度（滚转、俯仰、偏航）
  - 输出： $a_t = \text{ActorMLP}([f_t, a_{t-1}, a_{t-2}, a_{t-3}])$
    - $a_t \in \mathbb{R}^4$: 当前动作

4. 评论家网络
- **描述**: 输入CNN特征向量、前三步动作、当前动作及20维状态，输出动作价值评分。
- **公式**:  
  - 输入： $[f_t, a_{t-1}, a_{t-2}, a_{t-3}, a_t, s_t]$
    - $s_t$: 当前状态（20维）
  - 输出： $V_t = \text{CriticMLP}([f_t, a_{t-1}, a_{t-2}, a_{t-3}, a_t, s_t])$
    - $V_t$: 动作价值（标量，估计长期回报）


5. 强化学习训练
- **描述**: 使用PPO算法，通过评论家评分和奖励机制调整网络权重，训练4亿次交互。
- **公式**:  奖励： $r_t = r_t^{\text{prog}} + r_t^{\text{perc}} + r_t^{\text{pass}} - r_t^{\text{cmd}} - r_t^{\text{crash}}$
  
  - PPO优化目标（简化为概念）：调整权重 $\theta$ 最大化期望回报：
    - $J(\theta) = \mathbb{E} [r_t + \gamma V_{t+1}]$
    - $\theta$: 演员和评论家网络参数


### 真实运行
1. 生成掩码
- **描述**: 使用Swin-transformer从摄像头视频流（1280×720，60Hz）生成门框掩码（84×84）。
- **公式**: $o_t = \text{SwinTransformer}(I_t)$
    - $I_t$: 当前视频帧
    - $o_t$: 掩码（84×84）

1. 实时控制
- **描述**: 将掩码输入训练好的演员网络（加载仿真权重），结合前三步动作，输出当前动作，通过地面站传输（33毫秒延迟）控制无人机。
- **公式**:  
  - 输入： $[f_t, a_{t-1}, a_{t-2}, a_{t-3}]$
    - $f_t = \text{CNN}(o_t)$
  - 输出： $a_t = \text{ActorMLP}([f_t, a_{t-1}, a_{t-2}, a_{t-3}])$
    - $a_t$: 当前动作（4维）

## 💡 思考

### 苏黎世RPG的工作思路

🎯 苏黎世RPG端到端的工作，主要是考虑到传统的感知-状态估计-规划-控制 这种路径不仅硬件成本高、模块之间的联系没有合理的建模、且流水线式的工作会造成耗时

> 为解决问题，端到端的主要思路：
> 1. 简单的cnn网络提取特征向量
> 2. 特权网络， 一部分作为专家，拥有仿真环境中的所有信息且不带噪点； 另外一部分作为学生，学生通过特征向量和之前时刻的动作输出当前时刻的动作；专家对学生输出的动作进行打分；并且引入强化学习，结合打分和奖励策略不断优化权重，最终输出合理的动作（加速度+角速度）
> 3. 仿真到真实环境的迁移： 这里RPG用到了多种方法，例如抽样、保证输入一致零样本迁移


### 在智能体上的应用
> 端到端控制，可以快速响应； 比如， 用户给定一个点，能快速安全自主的从当前点飞到目标点


⭐️ 可应用在J事运送物资  
⭐️ 快速搜索目标  
⭐️ 探索位置环境并创建地图（自主巡检）  
⭐️ 机械臂自主抓取  
⭐️ 无人驾驶端到端， 从传感器输入直接映射成无人车的驾驶决策和控制输出


### 相关工作

[dronet](https://github.com/uzh-rpg/rpg_public_dronet)  
[pulp_dronet](https://github.com/pulp-platform/pulp-dronet)  
[敏捷飞行](https://github.com/uzh-rpg/agile_autonomy)  
[UAV_Navigation_DRL_AirSim](https://github.com/heleidsn/UAV_Navigation_DRL_AirSim)  
[感知与行动融合综述](https://mp.weixin.qq.com/s?__biz=MzkwMTgxNjk5OA==&mid=2247486626&idx=1&sn=8fe825d04c7a2d314787e8be85c7c56b&chksm=c10d4771c996a483b4daadfd11787aead6a0d5d828c87a232aee3a575bc0c713b6b57f0c8634&mpshare=1&scene=1&srcid=0314iYdAQKPLoBrAnQ4iTzOn&sharer_shareinfo=4bb45d30acbdea3089fe84c75019bf44&sharer_shareinfo_first=4bb45d30acbdea3089fe84c75019bf44#rd)  
[强化学习在无人机中的应用](https://zhuanlan.zhihu.com/p/14995482270)  
[20m/s端到端无人机自主导航](https://zhuanlan.zhihu.com/p/19620119071)  
[准J事应用](https://www.youtube.com/watch?v=HipTO_7mUOw)  