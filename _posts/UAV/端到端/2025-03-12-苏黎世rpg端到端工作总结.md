---
layout:     post


title:      苏黎世端到端工作总结


date:       2025-03-12


author:     phoenixZ


header-img: img/dji3.jpg


catalog: true


tags:
    - rpg

    - 端到端
---
## 🚀DroNet: Learning to Fly by Driving

**核心问题：** 如何让无人机仅依靠前视摄像头在城市环境中安全、平稳地飞行，同时保证计算效率以适应嵌入式系统。
![]({{ site.baseurl }}/img/dronet.png)

### 网络架构

**输入：**
单张灰度图像，尺寸为 200×200 像素，由无人机的前视摄像头实时捕获。选择灰度图是为了降低计算复杂度，同时保留足够的视觉信息。

**模型结构：**
基于 8 层残差网络（ResNet-8），包含卷积层、池化层和跳跃连接（skip connections）。设计灵感来源于 ResNet，但大幅精简以适应实时性和嵌入式硬件（如 NVIDIA Jetson TX1）。参数量较小，仅约 310K 个参数，比当时的最佳模型（如 VGG）小 80 倍。

**输出：**DroNet 同时预测两个值：

- **转向角度（Steering Angle）：** 一个连续值，表示无人机应向左或向右调整方向的幅度，用于保持路径跟随和避开障碍物。
- **碰撞概率（Collision Probability）：** 一个介于 0 到 1 的值，表示当前路径上存在障碍物的可能性，用于触发减速或悬停等安全行为。

**双输出设计：**网络的最后一层分为两个分支：

- 一个回归分支预测转向角度（均方误差损失）。
- 一个分类分支预测碰撞概率（交叉熵损失）。
  联合训练两个任务，确保模型同时关注路径规划和安全。

### 训练数据

**创新点：**直接在无人机上收集飞行数据既危险又昂贵，因此 DroNet 使用地面车辆数据训练：

- **转向角度数据：** 来自 Udacity 自动驾驶数据集，包含汽车在城市街道上的驾驶视频和对应的方向盘角度。
- **碰撞概率数据：** 通过骑自行车在城市中收集的自定义数据集，标注前方是否有障碍物（如行人、车辆）。

**数据处理：**
Udacity 数据提供图像-转向角度对，用于回归任务。自行车数据通过手动标注（障碍物存在与否）生成二分类标签。数据经过预处理（如灰度化、resize）以匹配网络输入。

**迁移假设：**
地面车辆的视角（低视角）和无人机的高空视角差异较大，但论文假设导航的基本规则（如避开障碍物、跟随路径）在两种场景中是相似的。这种跨领域迁移是 DroNet 的核心创新。

### 导航策略

**控制逻辑：**

- **转向：** 根据网络预测的转向角度调整无人机的偏航（yaw），保持路径或避障。
- **速度控制：** 基于碰撞概率：
  - 如果概率低于阈值（如 0.5），保持正常速度。
  - 如果概率高于阈值，减速或悬停。
- **高度：** 论文中未明确控制高度，通常假设固定高度飞行。

**实时性：**
DroNet 在 NVIDIA Jetson TX1 上以 20 FPS 运行，延迟低，适合嵌入式应用。

## 🚀敏捷飞行：Learning High-Speed Flight in the Wild

> 解决问题：
>
> 1. 传统感知->建图/状态估计->控制 的方法模块化导致通信延迟、误差累积、不适应高速飞行场景
> 2. 通过端到端的思路，增强了系统对噪声的鲁棒性，降低延迟
>
> ⭐️ 可用于智能体自主探索、配送等场景

![敏捷飞行]({{ site.baseurl }}/img/agile_auto.png)

### 输入数据

1. 深度图像： d ∈ R^(640×480)，黑白图，表示障碍物的远近。
2. 状态信息： 速度  

   $$
   \mathbf{v} \in \mathbb{R}^3
   $$

    （三维速度向量）姿态 
   $$
   q \in \mathbb{R}^9
   $$

    （旋转矩阵）。
3. 目标方向： 

   $$
   \omega \in \mathbb{R}^3
   $$

   ，一个归一化的向量，指向未来1秒的参考点（从参考轨迹来的）。

### 特权学习：老师生成示范路径

1. 老师利用仿真环境中3D点云数据和地图信息离线生成路径分布
2. 打分细节

- 成本函数：  c(τ) = ∫₀¹ (λc C_{collision}(τ(t)) + (τ(t) - τ_{ref}(t))ᵀ Q (τ(t) - τ_{ref}(t))) dt
- \( C_{collision} \): 当离障碍物 < 0.4米时，成本上升（二次函数）
- \([\tau - \tau_{ref}]\): 偏离参考轨迹的惩罚。

3. 采样
   - 用 M-H 算法试5万条路径，成本低的概率高（  
     $$
     P(\tau) = \frac{1}{Z} \exp(-c(\tau))
     $$

       ）
   - 输出一堆路径分布，不是单条“最好”，而是多种可能。

### 特权学习：学生模仿老师

1. Backbone
   使用 MobileNet-V3 从深度图提取特征（高效处理图像）。
2. 输入

- 深度图
- 速度
- 姿态
- 目标方向

3. 输出
   3条路径 $\tau_{n,k} \in \mathbb{R}^{10 \times 3}$ 和它们的碰撞成本。

**总结：用“赢家通吃的原则”，让预测贴近老师的最佳路径**

### 真实飞行

1. 利用训练好的权重，在真实环境中利用退避视界的原则，每秒24次预测路径
2. 过程
   - 双目相机生成深度图
   - 网络预测3条路径+成本
   - 挑成本最低的（若成本接近，选省力的）
   - 模型预测控制器（MPC）执行

## 🚀像素到控制: Demonstrating Agile Flight from Pixels without State Estimation

![像素到控制]({{ site.baseurl }}/img/像素坐标端到端.png)

> 解决问题： 省去中间状态估计再规划的步骤，提速并且可以在硬件上更简单

### 仿真训练

1. 仿真环境生成数据

- **描述**: 仿真环境生成门框掩码和无人机状态信息。掩码通过透视投影生成（84×84像素），计算效率高（100微秒/帧），噪声少；状态信息是20维向量，仅训练时用。
- **公式**:
  - 掩码生成无显式公式，基于透视投影：门框边缘离散为5点，投影到图像平面。
  - 状态向量： $s = [p, \tilde{R}, v, \omega, i, d]$
    - $p \in \mathbb{R}^3$: 位置
    - $\tilde{R} \in \mathbb{R}^6$: 姿态（旋转矩阵前两列）
    - $v \in \mathbb{R}^3$: 线速度
    - $\omega \in \mathbb{R}^3$: 角速度
    - $i \in \mathbb{R}^2$: 门框索引
    - $d \in \mathbb{R}^3$: 下一门框相对位置

2. 特征提取

- **描述**: 三层卷积神经网络（CNN）将门框掩码转化为低维特征向量，作为演员和评论家的共享输入。
- **公式**:
  - $f_t = \text{CNN}(o_t)$
    - $o_t$: 当前掩码（84×84）
    - $f_t$: 特征向量（维度未明确，假设为低维）

3. 演员网络

- **描述**: 输入CNN特征向量和前三步动作，输出当前动作（4维：推力+角速度）。
- **公式**:
  - 输入： $[f_t, a_{t-1}, a_{t-2}, a_{t-3}]$
    - $a_{t-i}$: 前第$i$步动作， $a_{t-i} = [c_{t-i}, \omega_{B,\text{ref}, t-i}]$
    - $c$: 质量归一化总推力
    - $\omega_{B,\text{ref}} \in \mathbb{R}^3$: 角速度（滚转、俯仰、偏航）
  - 输出： $a_t = \text{ActorMLP}([f_t, a_{t-1}, a_{t-2}, a_{t-3}])$
    - $a_t \in \mathbb{R}^4$: 当前动作

4. 评论家网络

- **描述**: 输入CNN特征向量、前三步动作、当前动作及20维状态，输出动作价值评分。
- **公式**:
  - 输入： $[f_t, a_{t-1}, a_{t-2}, a_{t-3}, a_t, s_t]$
    - $s_t$: 当前状态（20维）
  - 输出： $V_t = \text{CriticMLP}([f_t, a_{t-1}, a_{t-2}, a_{t-3}, a_t, s_t])$
    - $V_t$: 动作价值（标量，估计长期回报）

5. 强化学习训练

- **描述**: 使用PPO算法，通过评论家评分和奖励机制调整网络权重，训练4亿次交互。
- **公式**:  奖励： $r_t = r_t^{\text{prog}} + r_t^{\text{perc}} + r_t^{\text{pass}} - r_t^{\text{cmd}} - r_t^{\text{crash}}$

  - PPO优化目标（简化为概念）：调整权重 $\theta$ 最大化期望回报：
    - $J(\theta) = \mathbb{E} [r_t + \gamma V_{t+1}]$
    - $\theta$: 演员和评论家网络参数

### 真实运行

1. 生成掩码

- **描述**: 使用Swin-transformer从摄像头视频流（1280×720，60Hz）生成门框掩码（84×84）。
- **公式**: $o_t = \text{SwinTransformer}(I_t)$
  - $I_t$: 当前视频帧
  - $o_t$: 掩码（84×84）

1. 实时控制

- **描述**: 将掩码输入训练好的演员网络（加载仿真权重），结合前三步动作，输出当前动作，通过地面站传输（33毫秒延迟）控制无人机。
- **公式**:
  - 输入： $[f_t, a_{t-1}, a_{t-2}, a_{t-3}]$
    - $f_t = \text{CNN}(o_t)$
  - 输出： $a_t = \text{ActorMLP}([f_t, a_{t-1}, a_{t-2}, a_{t-3}])$
    - $a_t$: 当前动作（4维）


## 🚀 YOPO
![YOPO]({{ site.baseurl }}/img/yopo.jpg)

### 第一步：传统规划问题的痛点

想象一下，你要让一架无人机从 A 点飞到 B 点，中间有很多障碍物（比如树、墙）。传统方法会分几个阶段：

- **感知与建图**：用传感器（像激光雷达或摄像头）扫描周围，画一张地图。
- **前端路径搜索**：在地图上找一条大致可行的路线（比如用 A* 算法）。
- **后端优化**：把粗糙的路线调整成平滑的轨迹，让无人机飞得更稳。

这种分阶段的方法虽然可行，但很麻烦：
- 每步都要单独计算，速度慢。
- 如果传感器数据有噪声（比如模糊或不完整），整个规划就容易出错。

这篇论文的规划器（简称 **YOPO**）想解决这些问题，把所有步骤塞到一个神经网络里，**一次性搞定**。

---

### 第二步：问题建模——规划变成回归任务
![YOPO_res]({{ site.baseurl }}/img/yopo_res.png)

YOPO 的核心想法是：别老想着画地图、找路了，我们直接让神经网络“猜”出一条好路径。怎么猜呢？

- **轨迹表示**：它用数学上的“多项式轨迹”来描述无人机的飞行路径（比如二次或三次方程，平滑又好计算）。
- **回归任务**：网络的任务是预测一堆可能的轨迹（每条轨迹有自己的形状和方向），然后给每条轨迹打个分（比如“这条路撞墙了，分数低；这条路通畅，分数高”）。
- **空间分离**：为了覆盖所有可能性，它会同时预测好多条轨迹，像是撒网一样，确保总有一条能用。

**打个比方**：就像你在一个迷宫里，传统方法是先画地图再找路，而 YOPO 是直接扔出几十个“探路小人”，让神经网络预测哪个小人能走到终点。

---

### 第三步：运动基元——预设“候选方案”

为了让网络预测更高效，YOPO 用了一组“运动基元”（Motion Primitives）：

- **什么是运动基元**：简单说，就是一些预定义的“动作模板”。比如“直走 1 米”“左转 45 度”“上升 0.5 米”这样的基本动作。
- **怎么用**：网络不是凭空猜轨迹，而是基于这些模板稍作调整（比如偏移一点距离或角度），生成最终路径。
- **单次前向传播**：网络一次性输出所有候选轨迹的调整量（偏移）和评分，然后挑最好的那条再优化一下。

**类比一下**：就像你在玩赛车游戏，不是自己画路线，而是从一堆预设的“跑法”里挑一个，再微调一下刹车和方向。


👇假设无人机在 (0, 0, 0) 点，速度为零，运动基元可能是这样的：

- **基元 1**：直走 1 米，终点 (1, 0, 0)，轨迹 
  $$ x(t) = t $$
  
- **基元 2**：右转 45 度，终点 (0.7, 0.7, 0)，轨迹 
  $$ x(t) = t \cos(45^\circ t), \quad y(t) = t \sin(45^\circ t) $$
  
- **基元 3**：上升 0.5 米，终点 (0, 0, 0.5)，轨迹 
  $$ z(t) = 0.5t $$

这些基元就像“模板”，网络预测：

- **偏移量**：比如基元 1 的终点从 (1, 0, 0) 调整到 (1.2, 0.1, 0)。
- **评分**：基元 1 可能撞墙，得分 0.2；基元 3 通畅，得分 0.9。

---

### 第四步：指导学习——不用专家也能训练

传统神经网络训练需要“标准答案”（比如专家给出的完美路径），但在无人机规划里，哪来的那么多专家数据呢？YOPO 用了一种叫“指导学习”的聪明办法：

- **数值梯度**：它用数学方法算出当前轨迹的“缺点”（比如离障碍物太近、路径太抖），这些缺点就是“梯度”。
- **无监督训练**：网络根据这些梯度自己调整，不需要人类告诉它“应该怎么飞”。
- **好处**：省去了手动标注数据的麻烦，还能适应各种新环境。

**举个例子**：
假设网络预测了一条轨迹，直走 1 米，但前面有墙：

- **普通反向传播**：需要专家说“应该左转”，然后算误差，优化网络。
- **指导学习**：
  - 目标函数发现“撞墙了”，算出梯度（比如“往左偏移 0.5 米”）。
  - 这个梯度告诉网络：下次预测时，把轨迹偏左，评分也调整。
  - 反向传播把这个“偏左”的建议传回网络权重。

---

### 第五步：训练与推理——从“全知”到“真实”

- **训练时**：网络可以看到环境的“全貌”（比如完整的地图），就像作弊一样，知道哪里有障碍物。这样它能学到最好的规划方式。
- **推理时**：实际用的时候，网络只能靠传感器的数据（比如摄像头拍的模糊画面），没有完整的地图。但因为训练时学得好，它能根据不完美的信息猜出靠谱的路径。

**类比一下**：就像你在模拟器里练开车，能看到全景地图；但真上路时只能靠车窗看外面，YOPO 就是在模拟器里练好了“第六感”，现实中也能应付。
   

## 💡 思考

### 苏黎世RPG的工作思路

🎯 苏黎世RPG端到端的工作，主要是考虑到传统的感知-状态估计-规划-控制 这种路径不仅硬件成本高、模块之间的联系没有合理的建模、且流水线式的工作会造成耗时

> 为解决问题，端到端的主要思路：
>
> 1. 简单的cnn网络提取特征向量
> 2. 特权网络， 一部分作为专家，拥有仿真环境中的所有信息且不带噪点； 另外一部分作为学生，学生通过特征向量和之前时刻的动作输出当前时刻的动作；专家对学生输出的动作进行打分；并且引入强化学习，结合打分和奖励策略不断优化权重，最终输出合理的动作（加速度+角速度）
> 3. 仿真到真实环境的迁移： 这里RPG用到了多种方法，例如抽样、保证输入一致零样本迁移

### 在智能体上的应用

> 端到端控制，可以快速响应； 比如， 用户给定一个点，能快速安全自主的从当前点飞到目标点

⭐️ 可应用在J事运送物资
⭐️ 快速搜索目标
⭐️ 探索位置环境并创建地图（自主巡检）
⭐️ 机械臂自主抓取
⭐️ 无人驾驶端到端， 从传感器输入直接映射成无人车的驾驶决策和控制输出

### 相关工作

[dronet](https://github.com/uzh-rpg/rpg_public_dronet)
[pulp_dronet](https://github.com/pulp-platform/pulp-dronet)
[敏捷飞行](https://github.com/uzh-rpg/agile_autonomy)
[UAV_Navigation_DRL_AirSim](https://github.com/heleidsn/UAV_Navigation_DRL_AirSim)
[感知与行动融合综述](https://mp.weixin.qq.com/s?__biz=MzkwMTgxNjk5OA==&mid=2247486626&idx=1&sn=8fe825d04c7a2d314787e8be85c7c56b&chksm=c10d4771c996a483b4daadfd11787aead6a0d5d828c87a232aee3a575bc0c713b6b57f0c8634&mpshare=1&scene=1&srcid=0314iYdAQKPLoBrAnQ4iTzOn&sharer_shareinfo=4bb45d30acbdea3089fe84c75019bf44&sharer_shareinfo_first=4bb45d30acbdea3089fe84c75019bf44#rd)
[强化学习在无人机中的应用](https://zhuanlan.zhihu.com/p/14995482270)
[20m/s端到端无人机自主导航](https://zhuanlan.zhihu.com/p/19620119071)
[准J事](https://www.youtube.com/watch?v=HipTO_7mUOw)
