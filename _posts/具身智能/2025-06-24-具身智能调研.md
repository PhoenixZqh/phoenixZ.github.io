---
layout:     post

title:      transformer详解

date:       2025-06-23

author:     phoenixZ

header-img: img/oip11.jpeg

catalog: true

tags:
    - 具身智能
---

# 概述
1. 具身智能是实现通用人工智能(AGI)的关键
2. 在虚拟空间中的智能体通常称为非具身智能，在物理空间、能够与环境互动的成为具身人工智能
3. 具身智能是多莫模态大模型最理想的载体
4. 智能体必须充分理解语言指令中的人类意图，主动探索周围环境，并能够全面感知来自虚拟与现实环境中的多模态信息
5. 多模态大模型、视觉编码器、大语言模型、世界模型等技术创新共同赋能，使得具身智能体具备了复杂环境感知、人机自然交互以及任务可靠执行的综合能力 


# 具身感知

## Active Visual Perception
1. 未来的视觉感知发展方向是以具身为中心的视觉推理和社交智能。与仅在图像中识别物体不同，具有具身感知的智能体需要在物理世界中移动并与环境交互。这要求对3D空间和动态环境有更深入的理解。具身感知不仅涉及视觉感知和推理，还需要理解场景中的3D关系，并基于视觉信息预测和执行复杂任务。
2. 被动感知依然是实现主动探索的前提

## 3D Visual Grounding

## VLN
VLN 要求机器人一方面理解复杂且多样的视觉观测信息，另一方面能够解释不同层级的语言指令。

VLN 的输入通常由两部分组成：

视觉信息：可以是过往轨迹的视频，或一组历史与当前的观测图像；

自然语言指令：包括智能体需要抵达的目标，或期望完成的任务描述。

Action=M(O,H,I)

其中：

Action：所选择的动作或动作序列，

O：当前的视觉观测信息（Observation），

H：历史信息（History），

I：自然语言指令（Instruction），

M：为基于这三类信息进行决策的策略模型。


常用评价指标：
SR（Success Rate，成功率）：直接反映具身智能体的导航任务完成情况；

TL（Trajectory Length，轨迹长度）：衡量智能体执行任务所耗路径的长度，体现导航效率；

SPL（Success weighted by Path Length，路径加权成功率）：综合考虑成功率与路径效率，用于衡量智能体的整体表现。

任务：

1. 单一导航任务
2. 包含交互行为的导航任务
3. 需按顺序完成的多阶段导航任务

数据集：
Room2Room
Room4Room
VLN-ce


方法：
1. 基于记忆理解的方法（Memory-Understanding Based）
   
- 该类方法侧重于对环境的感知和理解，基于历史观测数据或轨迹设计模型，属于依赖过去经验进行决策的范式。
- 图结构学习（Graph-based learning）是基于记忆理解的方法的重要组成部分
- 图结构学习以图的形式表示导航过程，其中具身智能体在每个时间步获得的信息被编码为图中的节点。智能体利用全局或部分导航图信息，作为历史轨迹的表示。


2. 基于未来预测的方法（Future-Prediction Based）

这类方法更多关注对未来状态的建模、预测和理解，属于未来导向的学习方式。


## 触觉

估计、识别、操作


# 具身交互

## 具身问答
## 具身抓取

# 具身智能体
# Sim2Real