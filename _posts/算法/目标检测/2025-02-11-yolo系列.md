---
layout:     post
title:      YOLO系列学习
date:       2024-02-11
author:     phoenixZ
header-img: img/oip5.jpeg
catalog: true
tags:
    - 深度学习
    - 目标检测
---
# YOLOv1 🎯

## 网络结构 📊

![YOLOv1网络结构]({{ site.baseurl }}img/yolov1结构.png)

### 详细结构

| 层                | 参数                             | 输出尺寸    |
| :---------------- | :------------------------------- | :---------- |
| 输入层            | 448x448x3 RGB图像                | 448x448x3   |
| 卷积层1           | o7x7卷积核(stride=2), 64个卷积核 | 224x224x64  |
| 最大池化层1       | 2x2池化核(stride=2)              | 112x112x64  |
| 卷积层2           | 3x3卷积核(stride=1), 192个卷积核 | 112x112x192 |
| 最大池化层2       | 2x2池化核(stride=2)              | 56x56x192   |
| 卷积层3           | 1x1卷积核, 128个卷积核           | 56x56x128   |
| 卷积层4           | 3x3卷积核, 256个卷积核           | 56x56x256   |
| 卷积层5           | 1x1卷积核, 256个卷积核           | 56x56x256   |
| 卷积层6           | 3x3卷积核, 512个卷积核           | 56x56x512   |
| 最大池化层3       | 2x2池化核(stride=2)              | 28x28x512   |
| 卷积层7-14        | 交替使用1x1(512)和3x3(1024)卷积  | 28x28x1024  |
| 最大池化层4       | 2x2池化核(stride=2)              | 14x14x1024  |
| 卷积层15-20       | 交替使用1x1(512)和3x3(1024)卷积  | 14x14x1024  |
| 卷积层21-24       | 4个3x3卷积层(1024)               | 7x7x1024    |
| 全连接层1         | 4096个神经元                     | 4096        |
| 全连接层2(输出层) | 7x7x30                           | 7x7x30      |

### 网络结构说明 🔍

- **输入**：448x448x3的RGB图像
- **网络组成**：
  - 24个卷积层 + 2个全连接层
  - 前20个卷积层：预训练用于图像分类
  - 后4个卷积层和2个全连接层：用于检测
- **输出**：7x7x30的张量
  - 7x7：图像网格划分
  - 30 = 2*5 + 20
    - 2个边界框 × 5个值(x,y,w,h,confidence)
    - 20个类别的条件概率

## 工作流程 🔄

1. **图像预处理** 📸

   - 将图像缩放到480x480
2. **划分网格** 📏

   - 将图像划分成7x7网格
3. **网格预测** 🎯

   - 目标中心点落在网格中时，该网格负责预测
   - 每个网格输出两个bbox，带置信度
   - 预测网格包含各类目标的概率
4. **输出处理** 🔄

   - 计算类置信度：`confidence × class probability`
   - 使用NMS去除重复框，保留最优结果

## 损失函数 📊

YOLOv1使用综合损失函数：

$$
\begin{aligned}
\text{Loss} &= \lambda_{\text{coord}} \sum_{i=0}^{S^2} \sum_{j=0}^{B} \mathbb{1}_{ij}^{\text{obj}} \left[ (x_i - \hat{x}_i)^2 + (y_i - \hat{y}_i)^2 \right] \\
&+ \lambda_{\text{coord}} \sum_{i=0}^{S^2} \sum_{j=0}^{B} \mathbb{1}_{ij}^{\text{obj}} \left[ (\sqrt{w_i} - \sqrt{\hat{w}_i})^2 + (\sqrt{h_i} - \sqrt{\hat{h}_i})^2 \right] \\
&+ \sum_{i=0}^{S^2} \sum_{j=0}^{B} \mathbb{1}_{ij}^{\text{obj}} (C_i - \hat{C}_i)^2 \\
&+ \lambda_{\text{noobj}} \sum_{i=0}^{S^2} \sum_{j=0}^{B} \mathbb{1}_{ij}^{\text{noobj}} (C_i - \hat{C}_i)^2 \\
&+ \sum_{i=0}^{S^2} \mathbb{1}_{i}^{\text{obj}} \sum_{c \in \text{classes}} (p_i(c) - \hat{p}_i(c))^2
\end{aligned}
$$

其中：

- $S^2$ 是网格数量（7×7=49）
- $B$ 是每个网格预测的边界框数量（2个）
- $\mathbb{1}_{ij}^{\text{obj}}$ 表示第i个网格的第j个边界框是否负责预测目标
- $\mathbb{1}_{ij}^{\text{noobj}}$ 表示第i个网格的第j个边界框不负责预测目标
- $\lambda_{\text{coord}}=5$ 是定位损失的权重
- $\lambda_{\text{noobj}}=0.5$ 是不包含目标的边界框的置信度损失权重
- $x_i, y_i, w_i, h_i$ 是预测的边界框坐标
- $\hat{x}_i, \hat{y}_i, \hat{w}_i, \hat{h}_i$ 是真实的边界框坐标
- $C_i$ 是预测的置信度
- $\hat{C}_i$ 是真实的置信度
- $p_i(c)$ 是预测的类别概率
- $\hat{p}_i(c)$ 是真实的类别概率

## 主要特点 ✨

1. 🎯 将目标检测转化为回归问题
2. ⚡ 端到端训练，速度快
3. 🎨 全图特征用于预测，背景误检率低
4. ⚠️ 小目标检测效果较差
5. ⚠️ 密集目标检测效果较差

# YOLOv2 🎯

## 核心思想 💡

- 提升准确率
- 保留实时性
- 适应更多类别的检测

在保证v1实时性优势的前提下，解决v1存在的小目标检测困难、定位不准、类别召回差的问题。

## 网络结构与输出 📊

   ![]({{ site.baseurl }}/img/darknet-19.png)

### 输入与输出

- **输入**：416x416 (更大尺寸，更有利于小目标检测)
- **输出**：13x13x(5x5+20), 或125（COCO）

### 与 YOLOv1 的主要区别 ⚡

- 从原来的 7×7 网格变成 13×13，更细化
- 每个网格预测更多的框（从 2 个变成 5 个）
- 引入 Anchor Boxes（先验框）
  - 通过 K-Means 聚类得到宽高如 [1:1, 2:1, 1:2, 3:1, 1:3] 等
  - 网络学习相对 anchor 的偏移量，而不是框的绝对值
  - 更容易拟合各种尺度和比例，尤其是小目标、长目标、宽目标等特殊目标

## 工作流程 🔄

1. 输入图像：416x416
2. 通过DarkNet-19网络提取特征, 19表示卷积测层个数
3. 输出13x13网格，每个网格预测5个Anchor
4. 每个Anchor包含：
   - tx, ty, tw, th, to（偏移量 + 置信度）
   - class probabilities
5. 将预测框变回原图坐标
6. NMS过滤

## 损失函数 📈

> YOLOv2的损失结构与YOLOv1基本一致，但内部采用Anchor机制来分配正负样本和IOU匹配，更加合理，训练效果更好。

## 主要改进对比 YOLOv1 🚀

| 改进项                 | 解释                                                         | 带来的好处                       |
| :--------------------- | :----------------------------------------------------------- | :------------------------------- |
| 1. Anchor Boxes        | 借鉴Faster R-CNN的做法，每个网格预测多个先验框               | 提升定位能力，支持多个目标       |
| 2. IOU Loss选择负责框  | 不再是哪个框置信度高就选谁，而是哪个Anchor与GT IOU最大谁负责 | 减少匹配错误                     |
| 3. Batch Normalization | 添加到每一层                                                 | 加快收敛，提升精度               |
| 4. 高分辨率训练        | 先在224x224上训练，再fine-tune到448x448                      | 适应高分辨率输入                 |
| 5. Darknet-19          | 替代YOLOv1中的AlexNet风格结构                                | 更快更准                         |
| 6. Passthrough Layer   | 类似于U-Net，将浅层特征引入后面用于检测                      | 有助于检测小目标                 |
| 7. 多尺度训练          | 每10次迭代随机改变输入分辨率（320~608）                      | 模型适应不同图像大小，提升鲁棒性 |
| 8. YOLO9000            | 用WordTree同时在COCO+ImageNet上训练                          | 支持9000类实时检测               |

### Anchors原理

1. **聚类生成先验框**

   - 使用K-means聚类算法对训练集中的所有标注框进行聚类
   - 选择k=5个聚类中心作为先验框的宽高比
   - 聚类时使用IOU作为距离度量，而不是欧氏距离
   - 每个网格预测5个不同尺度和形状的anchor box
2. **目标框分配**

   - 对于每个目标框，计算其与所有anchor box的IOU
   - 将目标框分配给IOU最大的anchor box
   - 如果IOU小于阈值(通常为0.5)，则视为负样本
3. **预测偏移量**

   - 网络不再直接预测边界框的绝对坐标
   - 而是预测相对于anchor box的偏移量(tx, ty, tw, th)
   - 通过sigmoid函数将tx, ty限制在0-1之间
   - 使用指数函数处理tw, th，允许预测框比anchor box更大或更小

### Passthrough Layer

![]({{ site.baseurl }}/img/passthrouph.png)

Passthrough Layer（直通层）是YOLOv2中的一个重要创新，它的主要目的是将浅层特征与深层特征进行融合，以提升小目标的检测效果。

1. **维度变化**：

   - 输入：4×4×1 的特征图
   - 输出：2×2×4 的特征图
   - 宽高变成原来的1/2
   - 通道数变成原来的4倍
2. **工作原理**：

   - 将相邻的2×2区域重新排列到通道维度
   - 这样可以在不丢失信息的情况下，将空间信息转换为通道信息
   - 这种操作类似于"空间到深度"的转换
3. **实际应用**：

   - 在YOLOv2中，Passthrough Layer将26×26×512的特征图转换为13×13×2048
   - 然后将这个特征图与网络深层13×13×1024的特征图进行拼接
   - 最终得到13×13×3072的特征图，浅层信息包含更多的细节信息，有助于检测小目标

# YOLOv3🎯

## Darknet-53

![]({{ site.baseurl }}/img/darknet-53.png)

- 由53层卷积组成
- 引入残差结构，帮助训练更深的网络
- 结构上类似ResNet，提升特征提取能力 （卷积替代最大池化，更适合检测任务，因为卷积可以学习、保留的信息更强）
- 提供丰富的多尺度特征图给检测头

## 1. 目标边界框的预测

YOLOv3在三个不同尺度的特征图上进行预测，每个尺度都使用不同的anchor box集合。这种多尺度预测机制使得YOLOv3能够更好地检测不同大小的目标。

### 多尺度预测机制

![YOLOv3多尺度预测]({{ site.baseurl }}/img/yolov3-bbox.png)

| 特征图尺寸 | 检测目标 |
| ---------- | -------- |
| 13×13     | 大目标   |
| 26×26     | 中等目标 |
| 52×52     | 小目标   |

### Anchor Box设计

每个尺度使用3个anchor box，总共9个anchor box。这些anchor box是通过K-means聚类在COCO数据集上得到的：

| 尺度   | Anchor Box尺寸                 |
| ------ | ------------------------------ |
| 大尺度 | (116,90), (156,198), (373,326) |
| 中尺度 | (30,61), (62,45), (59,119)     |
| 小尺度 | (10,13), (16,30), (33,23)      |

### 边界框预测公式

对于每个预测框，网络输出5个值：

- tx, ty：中心点坐标的偏移量
- tw, th：宽高的缩放因子
- to：置信度分数

最终边界框的计算公式：

$$
\begin{aligned}
b_x &= \sigma(t_x) + c_x \\
b_y &= \sigma(t_y) + c_y \\
b_w &= p_w e^{t_w} \\
b_h &= p_h e^{t_h}
\end{aligned}
$$

其中：

- (cx, cy) 是网格单元的左上角坐标
- (pw, ph) 是anchor box的宽高
- σ 是sigmoid函数，将预测值压缩到(0,1)区间

### 预测过程

1. **特征提取**：输入图像通过Darknet-53网络提取特征
2. **多尺度预测**：在三个不同尺度的特征图上进行预测
3. **边界框生成**：每个网格单元预测3个边界框
4. **置信度计算**：使用sigmoid函数计算每个框的置信度
5. **类别预测**：使用独立的逻辑回归分类器预测每个类别

### 预测输出

每个预测框的输出维度为：

- 边界框坐标 (4个值)
- 置信度分数 (1个值)
- 类别概率 (80个值，COCO数据集)

因此，每个尺度的输出张量形状为：

- 13×13×255 (3×(5+80))
- 26×26×255
- 52×52×255

## 2. 正负样本匹配

![]({{ site.baseurl }}/img/yolov3-正负样本匹配.png)

### 匹配步骤

✅ **第一步**：将标注框与所有 anchor box 进行 IOU 匹配

- 对于每一个 ground truth 真实框，与所有9个 anchor box（3个尺度 × 3个anchor）计算IOU
- 找到IOU最大的那个anchor box，设为该目标的正样本anchor
- 这个anchor box所在的网格cell被认为是负责预测该目标的位置

📌 这一策略保证了每个目标至少有一个anchor被分配为正样本。

✅ **第二步**：只在匹配到的尺度上预测该目标

- 该anchor box属于哪个尺度（13x13/26x26/52x52），就在哪个尺度上学习这个目标
- YOLOv3使用手动设定的anchor尺寸（k-means聚类生成），不同尺度的anchor更适合对应大小的目标

### 样本定义

1. **正样本**

   - 被分配到一个ground truth框（即最大IOU匹配）
   - 在对应的尺度特征图上
   - 用于回归目标框位置(tx, ty, tw, th)、置信度预测和分类损失
2. **负样本**

   - 没有被分配任何ground truth的anchor box
   - IOU不高（<阈值，如0.5）
   - 只学习"没有物体"的置信度损失
3. **忽略样本（Ignore区域）**

   - 与目标IOU比较高（如>0.5），但不是IOU最大的那个
   - 这些非最佳匹配但又不够差的anchor会被标记为忽略（ignore）
   - 不参与正负样本的置信度损失计算
   - 避免对hard-negative样本惩罚过大

📌 这种"ignore"机制能缓解hard negative造成的误导性梯度问题。

## 损失定义

YOLOv3的损失函数由三部分组成：边界框回归损失、置信度损失和分类损失。

### 1. 边界框回归损失

使用均方误差(MSE)计算预测框与真实框之间的差异：

$$
L_{box} = \lambda_{coord} \sum_{i=0}^{S^2} \sum_{j=0}^{B} \mathbb{1}_{ij}^{obj} \left[ (x_i - \hat{x}_i)^2 + (y_i - \hat{y}_i)^2 + (w_i - \hat{w}_i)^2 + (h_i - \hat{h}_i)^2 \right]
$$

其中只对正样本anchor计算：

- $\lambda_{coord}$ 是坐标损失的权重系数
- $S^2$ 是特征图网格数
- $B$ 是每个网格预测的边界框数量
- $\mathbb{1}_{ij}^{obj}$ 表示第i个网格的第j个边界框是否负责预测目标
- $(x_i, y_i, w_i, h_i)$ 是预测的边界框坐标
- $(\hat{x}_i, \hat{y}_i, \hat{w}_i, \hat{h}_i)$ 是真实的边界框坐标

### 2. 置信度损失

使用二元交叉熵损失计算预测置信度与真实置信度的差异：

$$
L_{conf} = \sum_{i=0}^{S^2} \sum_{j=0}^{B} \mathbb{1}_{ij}^{obj} (C_i - \hat{C}_i)^2 + \lambda_{noobj} \sum_{i=0}^{S^2} \sum_{j=0}^{B} \mathbb{1}_{ij}^{noobj} (C_i - \hat{C}_i)^2
$$

其中：

- $\lambda_{noobj}$ 是不包含目标的边界框的置信度损失权重
- $C_i$ 是预测的置信度
- $\hat{C}_i$ 是真实的置信度
- $\mathbb{1}_{ij}^{noobj}$ 表示第i个网格的第j个边界框不包含预测目标为1， 包含则为0

### 3. 分类损失

使用二元交叉熵损失计算每个类别的预测概率：

$$
L_{cls} = \sum_{i=0}^{S^2} \mathbb{1}_{i}^{obj} \sum_{c \in classes} \left[ \hat{p}_i(c) \log(p_i(c)) + (1-\hat{p}_i(c)) \log(1-p_i(c)) \right]
$$

其中：

- $\mathbb{1}_{i}^{obj}$ 表示第i个网格是否包含目标
- $p_i(c)$ 是预测的类别c的概率
- $\hat{p}_i(c)$ 是真实的类别c的概率

### 总损失

最终的损失函数是三个部分的加权和：

$$
L_{total} = L_{box} + L_{conf} + L_{cls}
$$

## YOLOv3_SPP

1. Mosaic图像增强
   四张图片随机缩放裁剪然后进行拼接，增加了数据的多样性
2. SPP模块
   实现不同尺度的融合
   是一种金字塔结构的池化方式，它将不同尺寸的最大池化操作（如 5×5、9×9、13×13）作用在特征图上，然后拼接结果。
   ![]({{ site.baseurl }}/img/spp_模块.png)
3. IOU Loss
   ![]({{ site.baseurl }}/img/ciou.png)
   - GIOU原理
   ![]({{ site.baseurl }}/img/giou.png)
   - DIOU 原理
   ![]({{ site.baseurl }}/img/diou.png)
   - CIOU 原理, 优秀的回归定位损失应该考虑: **重叠面积, 中心点距离, 长宽比**
   ![]({{ site.baseurl }}/img/ciou1.png)

4. Focal Loss
   - 为什么需要Focal Loss?
     - 在目标检测中，正负样本不平衡是一个常见问题。一张图片中可能包含多个目标，但背景区域（负样本）通常远多于目标区域（正样本）
     - 传统的交叉熵损失对所有样本一视同仁，导致模型过度关注容易分类的负样本，而忽略难分类的正样本
     - 这会导致模型对正样本的检测能力不足，影响检测性能

   - Focal Loss原理
     - 在交叉熵损失的基础上引入一个调制因子$(1-p_t)^\gamma$
     - 当样本被正确分类时，$p_t$接近1，调制因子接近0，降低该样本的权重
     - 当样本被错误分类时，$p_t$接近0，调制因子接近1，保持该样本的权重
     - $\gamma$参数控制调制因子的强度，通常取2
     - 公式：$FL(p_t) = -\alpha_t(1-p_t)^\gamma \log(p_t)$
     - 其中$\alpha_t$是类别权重，用于平衡正负样本


# YOLOv5🎯

# YOLOv8

# YOLOv11

# YOLOE
