---
layout:     post
title:      YOLO系列学习
date:       2024-02-11
author:     phoenixZ
header-img: img/oip5.jpeg
catalog: true
tags:
    - 深度学习
    - 目标检测
---


# YOLOv1 🎯

## 网络结构 📊

![YOLOv1网络结构]({{ site.baseurl }}img/yolov1结构.png)

### 详细结构

| 层                | 参数                             | 输出尺寸    |
| :---------------- | :------------------------------- | :---------- |
| 输入层            | 448x448x3 RGB图像                | 448x448x3   |
| 卷积层1           | o7x7卷积核(stride=2), 64个卷积核 | 224x224x64  |
| 最大池化层1       | 2x2池化核(stride=2)              | 112x112x64  |
| 卷积层2           | 3x3卷积核(stride=1), 192个卷积核 | 112x112x192 |
| 最大池化层2       | 2x2池化核(stride=2)              | 56x56x192   |
| 卷积层3           | 1x1卷积核, 128个卷积核           | 56x56x128   |
| 卷积层4           | 3x3卷积核, 256个卷积核           | 56x56x256   |
| 卷积层5           | 1x1卷积核, 256个卷积核           | 56x56x256   |
| 卷积层6           | 3x3卷积核, 512个卷积核           | 56x56x512   |
| 最大池化层3       | 2x2池化核(stride=2)              | 28x28x512   |
| 卷积层7-14        | 交替使用1x1(512)和3x3(1024)卷积  | 28x28x1024  |
| 最大池化层4       | 2x2池化核(stride=2)              | 14x14x1024  |
| 卷积层15-20       | 交替使用1x1(512)和3x3(1024)卷积  | 14x14x1024  |
| 卷积层21-24       | 4个3x3卷积层(1024)               | 7x7x1024    |
| 全连接层1         | 4096个神经元                     | 4096        |
| 全连接层2(输出层) | 7x7x30                           | 7x7x30      |

### 网络结构说明 🔍

- **输入**：448x448x3的RGB图像
- **网络组成**：
  - 24个卷积层 + 2个全连接层
  - 前20个卷积层：预训练用于图像分类
  - 后4个卷积层和2个全连接层：用于检测
- **输出**：7x7x30的张量
  - 7x7：图像网格划分
  - 30 = 2*5 + 20
    - 2个边界框 × 5个值(x,y,w,h,confidence)
    - 20个类别的条件概率

## 工作流程 🔄

1. **图像预处理** 📸

   - 将图像缩放到480x480
2. **划分网格** 📏

   - 将图像划分成7x7网格
3. **网格预测** 🎯

   - 目标中心点落在网格中时，该网格负责预测
   - 每个网格输出两个bbox，带置信度
   - 预测网格包含各类目标的概率
4. **输出处理** 🔄

   - 计算类置信度：`confidence × class probability`
   - 使用NMS去除重复框，保留最优结果

## 损失函数 📊

YOLOv1使用综合损失函数：

$$
\begin{aligned}
\text{Loss} &= \lambda_{\text{coord}} \sum_{i=0}^{S^2} \sum_{j=0}^{B} \mathbb{1}_{ij}^{\text{obj}} \left[ (x_i - \hat{x}_i)^2 + (y_i - \hat{y}_i)^2 \right] \\
&+ \lambda_{\text{coord}} \sum_{i=0}^{S^2} \sum_{j=0}^{B} \mathbb{1}_{ij}^{\text{obj}} \left[ (\sqrt{w_i} - \sqrt{\hat{w}_i})^2 + (\sqrt{h_i} - \sqrt{\hat{h}_i})^2 \right] \\
&+ \sum_{i=0}^{S^2} \sum_{j=0}^{B} \mathbb{1}_{ij}^{\text{obj}} (C_i - \hat{C}_i)^2 \\
&+ \lambda_{\text{noobj}} \sum_{i=0}^{S^2} \sum_{j=0}^{B} \mathbb{1}_{ij}^{\text{noobj}} (C_i - \hat{C}_i)^2 \\
&+ \sum_{i=0}^{S^2} \mathbb{1}_{i}^{\text{obj}} \sum_{c \in \text{classes}} (p_i(c) - \hat{p}_i(c))^2
\end{aligned}
$$

其中：

- $S^2$ 是网格数量（7×7=49）
- $B$ 是每个网格预测的边界框数量（2个）
- $\mathbb{1}_{ij}^{\text{obj}}$ 表示第i个网格的第j个边界框是否负责预测目标
- $\mathbb{1}_{ij}^{\text{noobj}}$ 表示第i个网格的第j个边界框不负责预测目标
- $\lambda_{\text{coord}}=5$ 是定位损失的权重
- $\lambda_{\text{noobj}}=0.5$ 是不包含目标的边界框的置信度损失权重
- $x_i, y_i, w_i, h_i$ 是预测的边界框坐标
- $\hat{x}_i, \hat{y}_i, \hat{w}_i, \hat{h}_i$ 是真实的边界框坐标
- $C_i$ 是预测的置信度
- $\hat{C}_i$ 是真实的置信度
- $p_i(c)$ 是预测的类别概率
- $\hat{p}_i(c)$ 是真实的类别概率

## 主要特点 ✨

1. 🎯 将目标检测转化为回归问题
2. ⚡ 端到端训练，速度快
3. 🎨 全图特征用于预测，背景误检率低
4. ⚠️ 小目标检测效果较差
5. ⚠️ 密集目标检测效果较差

# YOLOv2 🎯

## 核心思想 💡

- 提升准确率
- 保留实时性
- 适应更多类别的检测

在保证v1实时性优势的前提下，解决v1存在的小目标检测困难、定位不准、类别召回差的问题。

## 网络结构与输出 📊

   ![]({{ site.baseurl }}/img/darknet-19.png)

### 输入与输出

- **输入**：416x416 (更大尺寸，更有利于小目标检测)
- **输出**：13x13x(5x5+20), 或125（COCO）

### 与 YOLOv1 的主要区别 ⚡

- 从原来的 7×7 网格变成 13×13，更细化
- 每个网格预测更多的框（从 2 个变成 5 个）
- 引入 Anchor Boxes（先验框）
  - 通过 K-Means 聚类得到宽高如 [1:1, 2:1, 1:2, 3:1, 1:3] 等
  - 网络学习相对 anchor 的偏移量，而不是框的绝对值
  - 更容易拟合各种尺度和比例，尤其是小目标、长目标、宽目标等特殊目标

## 工作流程 🔄

1. 输入图像：416x416
2. 通过DarkNet-19网络提取特征, 19表示卷积测层个数
3. 输出13x13网格，每个网格预测5个Anchor
4. 每个Anchor包含：
   - tx, ty, tw, th, to（偏移量 + 置信度）
   - class probabilities
5. 将预测框变回原图坐标
6. NMS过滤

## 损失函数 📈

> YOLOv2的损失结构与YOLOv1基本一致，但内部采用Anchor机制来分配正负样本和IOU匹配，更加合理，训练效果更好。

## 主要改进对比 YOLOv1 🚀

| 改进项                 | 解释                                                         | 带来的好处                       |
| :--------------------- | :----------------------------------------------------------- | :------------------------------- |
| 1. Anchor Boxes        | 借鉴Faster R-CNN的做法，每个网格预测多个先验框               | 提升定位能力，支持多个目标       |
| 2. IOU Loss选择负责框  | 不再是哪个框置信度高就选谁，而是哪个Anchor与GT IOU最大谁负责 | 减少匹配错误                     |
| 3. Batch Normalization | 添加到每一层                                                 | 加快收敛，提升精度               |
| 4. 高分辨率训练        | 先在224x224上训练，再fine-tune到448x448                      | 适应高分辨率输入                 |
| 5. Darknet-19          | 替代YOLOv1中的AlexNet风格结构                                | 更快更准                         |
| 6. Passthrough Layer   | 类似于U-Net，将浅层特征引入后面用于检测                      | 有助于检测小目标                 |
| 7. 多尺度训练          | 每10次迭代随机改变输入分辨率（320~608）                      | 模型适应不同图像大小，提升鲁棒性 |
| 8. YOLO9000            | 用WordTree同时在COCO+ImageNet上训练                          | 支持9000类实时检测               |

### Anchors原理

1. **聚类生成先验框**

   - 使用K-means聚类算法对训练集中的所有标注框进行聚类
   - 选择k=5个聚类中心作为先验框的宽高比
   - 聚类时使用IOU作为距离度量，而不是欧氏距离
   - 每个网格预测5个不同尺度和形状的anchor box
2. **目标框分配**

   - 对于每个目标框，计算其与所有anchor box的IOU
   - 将目标框分配给IOU最大的anchor box
   - 如果IOU小于阈值(通常为0.5)，则视为负样本
3. **预测偏移量**

   - 网络不再直接预测边界框的绝对坐标
   - 而是预测相对于anchor box的偏移量(tx, ty, tw, th)
   - 通过sigmoid函数将tx, ty限制在0-1之间
   - 使用指数函数处理tw, th，允许预测框比anchor box更大或更小

### Passthrough Layer

![]({{ site.baseurl }}/img/passthrouph.png)

Passthrough Layer（直通层）是YOLOv2中的一个重要创新，它的主要目的是将浅层特征与深层特征进行融合，以提升小目标的检测效果。

1. **维度变化**：

   - 输入：4×4×1 的特征图
   - 输出：2×2×4 的特征图
   - 宽高变成原来的1/2
   - 通道数变成原来的4倍
2. **工作原理**：

   - 将相邻的2×2区域重新排列到通道维度
   - 这样可以在不丢失信息的情况下，将空间信息转换为通道信息
   - 这种操作类似于"空间到深度"的转换
3. **实际应用**：

   - 在YOLOv2中，Passthrough Layer将26×26×512的特征图转换为13×13×2048
   - 然后将这个特征图与网络深层13×13×1024的特征图进行拼接
   - 最终得到13×13×3072的特征图，浅层信息包含更多的细节信息，有助于检测小目标

# YOLOv3

![]({{ site.baseurl }}/img/yolov3.png)

# YOLOv5 🚀

# YOLOv8

# YOLOv11

# YOLOE
