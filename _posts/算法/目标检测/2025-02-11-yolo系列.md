---
layout:     post
title:      YOLO系列学习
date:       2024-02-11
author:     phoenixZ
header-img: img/oip5.jpeg
catalog: true
tags:
    - 深度学习
    - 目标检测
---
# YOLOv1 🎯

## 网络结构 📊

![YOLOv1网络结构]({{ site.baseurl }}img/yolov1结构.png)

### 详细结构

| 层                | 参数                             | 输出尺寸    |
| :---------------- | :------------------------------- | :---------- |
| 输入层            | 448x448x3 RGB图像                | 448x448x3   |
| 卷积层1           | o7x7卷积核(stride=2), 64个卷积核 | 224x224x64  |
| 最大池化层1       | 2x2池化核(stride=2)              | 112x112x64  |
| 卷积层2           | 3x3卷积核(stride=1), 192个卷积核 | 112x112x192 |
| 最大池化层2       | 2x2池化核(stride=2)              | 56x56x192   |
| 卷积层3           | 1x1卷积核, 128个卷积核           | 56x56x128   |
| 卷积层4           | 3x3卷积核, 256个卷积核           | 56x56x256   |
| 卷积层5           | 1x1卷积核, 256个卷积核           | 56x56x256   |
| 卷积层6           | 3x3卷积核, 512个卷积核           | 56x56x512   |
| 最大池化层3       | 2x2池化核(stride=2)              | 28x28x512   |
| 卷积层7-14        | 交替使用1x1(512)和3x3(1024)卷积  | 28x28x1024  |
| 最大池化层4       | 2x2池化核(stride=2)              | 14x14x1024  |
| 卷积层15-20       | 交替使用1x1(512)和3x3(1024)卷积  | 14x14x1024  |
| 卷积层21-24       | 4个3x3卷积层(1024)               | 7x7x1024    |
| 全连接层1         | 4096个神经元                     | 4096        |
| 全连接层2(输出层) | 7x7x30                           | 7x7x30      |

### 网络结构说明 🔍

- **输入**：448x448x3的RGB图像
- **网络组成**：
  - 24个卷积层 + 2个全连接层
  - 前20个卷积层：预训练用于图像分类
  - 后4个卷积层和2个全连接层：用于检测
- **输出**：7x7x30的张量
  - 7x7：图像网格划分
  - 30 = 2*5 + 20
    - 2个边界框 × 5个值(x,y,w,h,confidence)
    - 20个类别的条件概率

## 工作流程 🔄

1. **图像预处理** 📸

   - 将图像缩放到480x480
2. **划分网格** 📏

   - 将图像划分成7x7网格
3. **网格预测** 🎯

   - 目标中心点落在网格中时，该网格负责预测
   - 每个网格输出两个bbox，带置信度
   - 预测网格包含各类目标的概率
4. **输出处理** 🔄

   - 计算类置信度：`confidence × class probability`
   - 使用NMS去除重复框，保留最优结果

## 损失函数 📊

YOLOv1使用综合损失函数：

$$
\begin{aligned}
\text{Loss} &= \lambda_{\text{coord}} \sum_{i=0}^{S^2} \sum_{j=0}^{B} \mathbb{1}_{ij}^{\text{obj}} \left[ (x_i - \hat{x}_i)^2 + (y_i - \hat{y}_i)^2 \right] \\
&+ \lambda_{\text{coord}} \sum_{i=0}^{S^2} \sum_{j=0}^{B} \mathbb{1}_{ij}^{\text{obj}} \left[ (\sqrt{w_i} - \sqrt{\hat{w}_i})^2 + (\sqrt{h_i} - \sqrt{\hat{h}_i})^2 \right] \\
&+ \sum_{i=0}^{S^2} \sum_{j=0}^{B} \mathbb{1}_{ij}^{\text{obj}} (C_i - \hat{C}_i)^2 \\
&+ \lambda_{\text{noobj}} \sum_{i=0}^{S^2} \sum_{j=0}^{B} \mathbb{1}_{ij}^{\text{noobj}} (C_i - \hat{C}_i)^2 \\
&+ \sum_{i=0}^{S^2} \mathbb{1}_{i}^{\text{obj}} \sum_{c \in \text{classes}} (p_i(c) - \hat{p}_i(c))^2
\end{aligned}
$$

其中：

- $S^2$ 是网格数量（7×7=49）
- $B$ 是每个网格预测的边界框数量（2个）
- $\mathbb{1}_{ij}^{\text{obj}}$ 表示第i个网格的第j个边界框是否负责预测目标
- $\mathbb{1}_{ij}^{\text{noobj}}$ 表示第i个网格的第j个边界框不负责预测目标
- $\lambda_{\text{coord}}=5$ 是定位损失的权重
- $\lambda_{\text{noobj}}=0.5$ 是不包含目标的边界框的置信度损失权重
- $x_i, y_i, w_i, h_i$ 是预测的边界框坐标
- $\hat{x}_i, \hat{y}_i, \hat{w}_i, \hat{h}_i$ 是真实的边界框坐标
- $C_i$ 是预测的置信度
- $\hat{C}_i$ 是真实的置信度
- $p_i(c)$ 是预测的类别概率
- $\hat{p}_i(c)$ 是真实的类别概率

## 主要特点 ✨

1. 🎯 将目标检测转化为回归问题
2. ⚡ 端到端训练，速度快
3. 🎨 全图特征用于预测，背景误检率低
4. ⚠️ 小目标检测效果较差
5. ⚠️ 密集目标检测效果较差

# YOLOv2 🎯

## 核心思想 💡

- 提升准确率
- 保留实时性
- 适应更多类别的检测

在保证v1实时性优势的前提下，解决v1存在的小目标检测困难、定位不准、类别召回差的问题。

## 网络结构与输出 📊

   ![]({{ site.baseurl }}/img/darknet-19.png)

### 输入与输出

- **输入**：416x416 (更大尺寸，更有利于小目标检测)
- **输出**：13x13x(5x5+20), 或125（COCO）

### 与 YOLOv1 的主要区别 ⚡

- 从原来的 7×7 网格变成 13×13，更细化
- 每个网格预测更多的框（从 2 个变成 5 个）
- 引入 Anchor Boxes（先验框）
  - 通过 K-Means 聚类得到宽高如 [1:1, 2:1, 1:2, 3:1, 1:3] 等
  - 网络学习相对 anchor 的偏移量，而不是框的绝对值
  - 更容易拟合各种尺度和比例，尤其是小目标、长目标、宽目标等特殊目标

## 工作流程 🔄

1. 输入图像：416x416
2. 通过DarkNet-19网络提取特征, 19表示卷积测层个数
3. 输出13x13网格，每个网格预测5个Anchor
4. 每个Anchor包含：
   - tx, ty, tw, th, to（偏移量 + 置信度）
   - class probabilities
5. 将预测框变回原图坐标
6. NMS过滤

## 损失函数 📈

> YOLOv2的损失结构与YOLOv1基本一致，但内部采用Anchor机制来分配正负样本和IOU匹配，更加合理，训练效果更好。

## 主要改进对比 YOLOv1 🚀

| 改进项                 | 解释                                                         | 带来的好处                       |
| :--------------------- | :----------------------------------------------------------- | :------------------------------- |
| 1. Anchor Boxes        | 借鉴Faster R-CNN的做法，每个网格预测多个先验框               | 提升定位能力，支持多个目标       |
| 2. IOU Loss选择负责框  | 不再是哪个框置信度高就选谁，而是哪个Anchor与GT IOU最大谁负责 | 减少匹配错误                     |
| 3. Batch Normalization | 添加到每一层                                                 | 加快收敛，提升精度               |
| 4. 高分辨率训练        | 先在224x224上训练，再fine-tune到448x448                      | 适应高分辨率输入                 |
| 5. Darknet-19          | 替代YOLOv1中的AlexNet风格结构                                | 更快更准                         |
| 6. Passthrough Layer   | 类似于U-Net，将浅层特征引入后面用于检测                      | 有助于检测小目标                 |
| 7. 多尺度训练          | 每10次迭代随机改变输入分辨率（320~608）                      | 模型适应不同图像大小，提升鲁棒性 |
| 8. YOLO9000            | 用WordTree同时在COCO+ImageNet上训练                          | 支持9000类实时检测               |

### Anchors原理

1. **聚类生成先验框**

   - 使用K-means聚类算法对训练集中的所有标注框进行聚类
   - 选择k=5个聚类中心作为先验框的宽高比
   - 聚类时使用IOU作为距离度量，而不是欧氏距离
   - 每个网格预测5个不同尺度和形状的anchor box
2. **目标框分配**

   - 对于每个目标框，计算其与所有anchor box的IOU
   - 将目标框分配给IOU最大的anchor box
   - 如果IOU小于阈值(通常为0.5)，则视为负样本
3. **预测偏移量**

   - 网络不再直接预测边界框的绝对坐标
   - 而是预测相对于anchor box的偏移量(tx, ty, tw, th)
   - 通过sigmoid函数将tx, ty限制在0-1之间
   - 使用指数函数处理tw, th，允许预测框比anchor box更大或更小

### Passthrough Layer

![]({{ site.baseurl }}/img/passthrouph.png)

Passthrough Layer（直通层）是YOLOv2中的一个重要创新，它的主要目的是将浅层特征与深层特征进行融合，以提升小目标的检测效果。

1. **维度变化**：

   - 输入：4×4×1 的特征图
   - 输出：2×2×4 的特征图
   - 宽高变成原来的1/2
   - 通道数变成原来的4倍
2. **工作原理**：

   - 将相邻的2×2区域重新排列到通道维度
   - 这样可以在不丢失信息的情况下，将空间信息转换为通道信息
   - 这种操作类似于"空间到深度"的转换
3. **实际应用**：

   - 在YOLOv2中，Passthrough Layer将26×26×512的特征图转换为13×13×2048
   - 然后将这个特征图与网络深层13×13×1024的特征图进行拼接
   - 最终得到13×13×3072的特征图，浅层信息包含更多的细节信息，有助于检测小目标

# YOLOv3🎯

![]({{ site.baseurl }}/img/yolov3_db.jpg)

## Darknet-53

![]({{ site.baseurl }}/img/darknet-53.png)

- 由53层卷积组成
- 引入残差结构，帮助训练更深的网络
- 结构上类似ResNet，提升特征提取能力 （卷积替代最大池化，更适合检测任务，因为卷积可以学习、保留的信息更强）
- 提供丰富的多尺度特征图给检测头

## 1. 目标边界框的预测

YOLOv3在三个不同尺度的特征图上进行预测，每个尺度都使用不同的anchor box集合。这种多尺度预测机制使得YOLOv3能够更好地检测不同大小的目标。

### 多尺度预测机制

![YOLOv3多尺度预测]({{ site.baseurl }}/img/yolov3-bbox.png)

| 特征图尺寸 | 检测目标 |
| ---------- | -------- |
| 13×13     | 大目标   |
| 26×26     | 中等目标 |
| 52×52     | 小目标   |

### Anchor Box设计

每个尺度使用3个anchor box，总共9个anchor box。这些anchor box是通过K-means聚类在COCO数据集上得到的：

| 尺度   | Anchor Box尺寸                 |
| ------ | ------------------------------ |
| 大尺度 | (116,90), (156,198), (373,326) |
| 中尺度 | (30,61), (62,45), (59,119)     |
| 小尺度 | (10,13), (16,30), (33,23)      |

### 边界框预测公式

对于每个预测框，网络输出5个值：

- tx, ty：中心点坐标的偏移量
- tw, th：宽高的缩放因子
- to：置信度分数

最终边界框的计算公式：

$$
\begin{aligned}
b_x &= \sigma(t_x) + c_x \\
b_y &= \sigma(t_y) + c_y \\
b_w &= p_w e^{t_w} \\
b_h &= p_h e^{t_h}
\end{aligned}
$$

其中：

- (cx, cy) 是网格单元的左上角坐标
- (pw, ph) 是anchor box的宽高
- σ 是sigmoid函数，将预测值压缩到(0,1)区间

### 预测过程

1. **特征提取**：输入图像通过Darknet-53网络提取特征
2. **多尺度预测**：在三个不同尺度的特征图上进行预测
3. **边界框生成**：每个网格单元预测3个边界框
4. **置信度计算**：使用sigmoid函数计算每个框的置信度
5. **类别预测**：使用独立的逻辑回归分类器预测每个类别

### 预测输出

每个预测框的输出维度为：

- 边界框坐标 (4个值)
- 置信度分数 (1个值)
- 类别概率 (80个值，COCO数据集)

因此，每个尺度的输出张量形状为：

- 13×13×255 (3×(5+80))
- 26×26×255
- 52×52×255

## 2. 正负样本匹配

![]({{ site.baseurl }}/img/yolov3-正负样本匹配.png)

### 匹配步骤

✅ **第一步**：将标注框与所有 anchor box 进行 IOU 匹配

- 对于每一个 ground truth 真实框，与所有9个 anchor box（3个尺度 × 3个anchor）计算IOU
- 找到IOU最大的那个anchor box，设为该目标的正样本anchor
- 这个anchor box所在的网格cell被认为是负责预测该目标的位置

📌 这一策略保证了每个目标至少有一个anchor被分配为正样本。

✅ **第二步**：只在匹配到的尺度上预测该目标

- 该anchor box属于哪个尺度（13x13/26x26/52x52），就在哪个尺度上学习这个目标
- YOLOv3使用手动设定的anchor尺寸（k-means聚类生成），不同尺度的anchor更适合对应大小的目标

### 样本定义

1. **正样本**

   - 被分配到一个ground truth框（即最大IOU匹配）
   - 在对应的尺度特征图上
   - 用于回归目标框位置(tx, ty, tw, th)、置信度预测和分类损失
2. **负样本**

   - 没有被分配任何ground truth的anchor box
   - IOU不高（<阈值，如0.5）
   - 只学习"没有物体"的置信度损失
3. **忽略样本（Ignore区域）**

   - 与目标IOU比较高（如>0.5），但不是IOU最大的那个
   - 这些非最佳匹配但又不够差的anchor会被标记为忽略（ignore）
   - 不参与正负样本的置信度损失计算
   - 避免对hard-negative样本惩罚过大

📌 这种"ignore"机制能缓解hard negative造成的误导性梯度问题。

## 损失定义

YOLOv3的损失函数由三部分组成：边界框回归损失、置信度损失和分类损失。

### 1. 边界框回归损失

使用均方误差(MSE)计算预测框与真实框之间的差异：

$$
L_{box} = \lambda_{coord} \sum_{i=0}^{S^2} \sum_{j=0}^{B} \mathbb{1}_{ij}^{obj} \left[ (x_i - \hat{x}_i)^2 + (y_i - \hat{y}_i)^2 + (w_i - \hat{w}_i)^2 + (h_i - \hat{h}_i)^2 \right]
$$

其中只对正样本anchor计算：

- $\lambda_{coord}$ 是坐标损失的权重系数
- $S^2$ 是特征图网格数
- $B$ 是每个网格预测的边界框数量
- $\mathbb{1}_{ij}^{obj}$ 表示第i个网格的第j个边界框是否负责预测目标
- $(x_i, y_i, w_i, h_i)$ 是预测的边界框坐标
- $(\hat{x}_i, \hat{y}_i, \hat{w}_i, \hat{h}_i)$ 是真实的边界框坐标

### 2. 置信度损失

使用二元交叉熵损失计算预测置信度与真实置信度的差异：

$$
L_{conf} = \sum_{i=0}^{S^2} \sum_{j=0}^{B} \mathbb{1}_{ij}^{obj} (C_i - \hat{C}_i)^2 + \lambda_{noobj} \sum_{i=0}^{S^2} \sum_{j=0}^{B} \mathbb{1}_{ij}^{noobj} (C_i - \hat{C}_i)^2
$$

其中：

- $\lambda_{noobj}$ 是不包含目标的边界框的置信度损失权重
- $C_i$ 是预测的置信度
- $\hat{C}_i$ 是真实的置信度
- $\mathbb{1}_{ij}^{noobj}$ 表示第i个网格的第j个边界框不包含预测目标为1， 包含则为0

### 3. 分类损失

使用二元交叉熵损失计算每个类别的预测概率：

$$
L_{cls} = \sum_{i=0}^{S^2} \mathbb{1}_{i}^{obj} \sum_{c \in classes} \left[ \hat{p}_i(c) \log(p_i(c)) + (1-\hat{p}_i(c)) \log(1-p_i(c)) \right]
$$

其中：

- $\mathbb{1}_{i}^{obj}$ 表示第i个网格是否包含目标
- $p_i(c)$ 是预测的类别c的概率
- $\hat{p}_i(c)$ 是真实的类别c的概率

### 总损失

最终的损失函数是三个部分的加权和：

$$
L_{total} = L_{box} + L_{conf} + L_{cls}
$$

## YOLOv3_SPP

1. Mosaic图像增强
   四张图片随机缩放裁剪然后进行拼接，增加了数据的多样性
2. SPP模块
   实现不同尺度的融合
   是一种金字塔结构的池化方式，它将不同尺寸的最大池化操作（如 5×5、9×9、13×13）作用在特征图上，然后拼接结果。
   ![]({{ site.baseurl }}/img/spp_模块.png)
3. IOU Loss
   ![]({{ site.baseurl }}/img/ciou.png)

   - GIOU原理
     ![]({{ site.baseurl }}/img/giou.png)
   - DIOU 原理
     ![]({{ site.baseurl }}/img/diou.png)
   - CIOU 原理, 优秀的回归定位损失应该考虑: **重叠面积, 中心点距离, 长宽比**
     ![]({{ site.baseurl }}/img/ciou1.png)
4. Focal Loss

   - 为什么需要Focal Loss?

     - 在目标检测中，正负样本不平衡是一个常见问题。一张图片中可能包含多个目标，但背景区域（负样本）通常远多于目标区域（正样本）
     - 传统的交叉熵损失对所有样本一视同仁，导致模型过度关注容易分类的负样本，而忽略难分类的正样本
     - 这会导致模型对正样本的检测能力不足，影响检测性能
   - Focal Loss原理

     - 在交叉熵损失的基础上引入一个调制因子$(1-p_t)^\gamma$
     - 当样本被正确分类时，$p_t$接近1，调制因子接近0，降低该样本的权重
     - 当样本被错误分类时，$p_t$接近0，调制因子接近1，保持该样本的权重
     - $\gamma$参数控制调制因子的强度，通常取2
     - 公式：$FL(p_t) = -\alpha_t(1-p_t)^\gamma \log(p_t)$
     - 其中$\alpha_t$是类别权重，用于平衡正负样本

# YOLOv5 🎯

## 网络结构 📊

### 整体架构图 🏗️

1. 大白图
   ![]({{ site.baseurl }}/img/yolov5_db.jpg)
2. 详细结构图
   ![]({{ site.baseurl }}/img/yolov5结构.png)

### Backbone 🦴

> backbone的作用是提取图像有用的特征
> 浅层: 关注细节(边缘|纹理)
> 深层: 关注语义(轮廓|形状|物体类别)

#### 1. C3 模块 🔄

- 是CSP结构的实现
- 将输入分为两部分:
  - 一部分通过Bottleneck
  - 另一部分直接传递
  - 最后将两部分concat在一起

优点 ✨:

- 降低了计算量
- 同时保留丰富的语义信息

#### 2. Bottleneck 模块 🍾

- 包含多个卷积层(1x1, 3x3, 1x1)
- 通过残差连接(加法操作)增强梯度传播
- 提高网格深度和性能

#### 3. 特征提取流程 P1-P5 📈

| 层 | 输入尺寸    | 操作       | 输出尺寸    |
| :- | :---------- | :--------- | :---------- |
| P1 | 640x640     | CBS下采样  | 320x320x64  |
| P2 | 320x320x64  | CBS下采样  | 160x160x128 |
| C3 | 160x160x128 | C3模块处理 | 160x160x192 |
| P3 | 160x160x192 | CBS        | 80x80x256   |
| C3 | 80x80x256   | C3模块处理 | 80x80x256   |
| P4 | 80x80x256   | CBS        | 40x40x512   |
| C3 | 40x40x512   | C3模块处理 | 40x40x512   |
| P5 | 40x40x512   | CBS        | 20x20x1024  |
| C3 | 20x20x1024  | C3模块处理 | 20x20x1024  |

> 💡 为什么要先降维再升维?
>
> - 降维: 减少计算量
> - 升维: 增加特征图的通道数, 提高特征图的表达能力

### Neck 🦒

> 将来自不同深度的特征图进行融合,使得检测头同时具备浅层(高分辨率) 和深层(语义)的特征

#### 1. SPPF (Spatial Pyramid Pooling - Fast) 🏊

##### SPP模块

![]({{ site.baseurl }}/img/SPP.png)

- 输入: 20x20x1024
- 多尺度池化输出保持不变:
  - 5x5: 20x20x1024x5x5 = 10.24M
  - 9x9: 20x20x1024x9x9 = 33.18M
  - 13x13: 20x20x1024x13x13 = 69.21M

> 💭 为什么使用不同的核大小?
>
> - 不改变空间尺寸, 改变感受野范围
> - 感受野小: 捕捉局部特征(边缘)
> - 感受野中: 捕捉中等结构(小目标)
> - 感受野大: 捕捉全局特征(背景/大目标)

缺点 ⚠️:

- 计算量大
- 不同池化核的感受野可能有重叠, 导致特征冗余

##### SPPF结构

![]({{ site.baseurl }}/img/SPPF.png)

- 输入: 20x20x1024
- 串行池化:
  - 第一次5x5: 感受野为5x5
  - 第二次5x5: 感受野为9x9
  - 第三次5x5: 感受野为13x13
  - 总计算量: 3x(20x20x1024x5x5) = 30.72M

优点 ✨:

- 计算量低
- 结构简单

#### 2. FPN (Feature Pyramid Network) 🔻🔺

![]({{ site.baseurl }}/img/fpn_pan.png)


> 特征自上而下融合，高层引导低层

路径 📍:

SPPF（20x20x512）
→ 上采样（Upsample）
→ 与 P4（40x40x512）Concat
→ C3 模块（输出 40x40x512）
→ 上采样
→ 与 P3（80x80x256）Concat
→ C3 模块（输出 80x80x256）

作用 :

- 把深层语义特征（SPPF 输出）传递到浅层
- 增强小目标感知
- 上采样、Concat 后接 C3 模块增强特征融合

#### 3. PAN (Path Aggregation Network) 🔺

> 自底向上路径增强，浅层引导深层

路径 📍:

80x80x256
→ 下采样（Conv + stride=2）
→ 与前一层的 40x40x512 Concat
→ C3 模块（输出 40x40x512）
→ 下采样
→ 与 20x20x512 Concat
→ C3 模块（输出 20x20x512）

作用 🎯:

- 引入更多位置细节信息（如边缘、纹理）给深层特征
- 提升检测准确率，尤其对中大目标效果好

### Head

> 负责从neck部分输出的多尺度特征图中预测每个anchor框的中心位置偏移,宽高,置信度和类别, 是整个模型进行目标检测的"输出接口"

#### 1. 多尺度检测 📏

- 小目标(80x80)
- 中目标(40x40)
- 大目标(20x20)

> 💭 为什么大尺度的特征图用来预测小目标?

- 因为具有更高的空间分辨率, 可以更细致的看到像素(看到更小的像素点)
- 以80x80的特征图为例, 640 / 80 = 8px, 可以看到原图8个像素

#### 2. Anchor-based 预测

* Anchor 是 YOLOv5 中预定义的一组边界框（bounding box），每个 anchor 有一个固定的宽高比（aspect ratio）和尺寸。
* 这些 anchor 是基于数据集的统计特性（通过 K-means 聚类或 AutoAnchor 自动优化）预设的，用于适配不同形状和大小的目标
* 对于 640x640 输入，YOLOv5 默认在 3 个尺度（80x80、40x40、20x20）上各定义 3 个 anchor，总共 9 个 anchor 模板

输出格式 📝:

[tx, ty, tw, th, obj_conf, class1, class2, ..., classN]

- tx, ty: 表示边界框中心点相对于 grid cell 左上角的相对偏移量，经过 sigmoid 函数归一化到 [0, 1]，表示在当前 grid cell 内的偏移比例
- tw, th: 表示边界框宽高相对于 anchor 尺寸的缩放因子（对数空间）
- obj_conf: 目标存在的置信度, 表示该anchor是否存在目标的概率
- class1~N: 对每个类别的分类置信度

#### 3. 输出预测结果 📊

**Head 输出形状**:

[batch_size, grid_size, grid_size, num_anchors, 5 + num_classes]

**解码过程** 🔄:

$b_x = \sigma(t_x) + c_x$

$b_y = \sigma(t_y) + c_y$

$b_w = anchor_w \cdot e^{t_w}$

$b_h = anchor_h \cdot e^{t_h}$

**解码过程示例** 📝

| 参数     | 值                   | 说明                           |
| :------- | :------------------- | :----------------------------- |
| 图像大小 | 640×640             | 原始输入图像尺寸               |
| 输出层   | 80×80               | 特征图尺寸，每个cell大小为8×8 |
| Anchor   | (40, 60)             | 当前使用的anchor框尺寸         |
| 网络输出 | (0.5, 0.5, 0.2, 0.1) | (tx, ty, tw, th)预测值         |
| Grid位置 | (20, 35)             | 当前cell的坐标(cx, cy)         |

- 中心点坐标计算：

  - $b_x = \sigma(0.5) + 20 \approx 0.62 + 20 \approx 20.62$
  - $b_y = \sigma(0.5) + 35 \approx 0.62 + 35 \approx 35.62$
- 宽高计算：

  - $b_w = 40 \cdot e^{0.2} \approx 40 \cdot 1.22 \approx 48.8$
  - $b_h = 60 \cdot e^{0.1} \approx 60 \cdot 1.11 \approx 66.6$
- 最终坐标：

  - 将上述坐标乘以stride(8)得到图像中的实际像素位置
  - 最终框位置：(165, 285, 390, 533)

> 💡 说明：
>
> - sigmoid函数将tx, ty限制在(0,1)范围内
> - exp函数允许预测框比anchor更大或更小
> - stride用于将特征图坐标映射回原图坐标

## 数据增强 🎨

### 1. Mosaic 数据增强 🎭

- 将4张图片随机缩放、裁剪后拼接成一张图片
- 优点：
  - 增加数据的多样性
  - 丰富目标的尺度、场景
  - 减少对batch size的需求
  - 增强模型对不同尺度的适应能力

### 2. Copy-Paste 增强 📋

- 将目标从一张图片复制到另一张图片
- 优点：
  - 增加目标数量
  - 提高小目标检测能力
  - 增强模型对遮挡的鲁棒性

### 3. Random Affine 变换 🔄

- 随机旋转、平移、缩放
- 优点：
  - 增加数据多样性
  - 提高模型对视角变化的鲁棒性

### 4. Albumentations 增强

- 包含多种图像增强方法：
  - 色彩抖动
  - 模糊
  - 噪声
  - 对比度调整
  - 亮度调整
- 优点：
  - 提高模型对图像质量变化的鲁棒性
  - 增强模型泛化能力

## 训练策略 📈

### 1. 多尺度训练 🔍

- 训练时随机改变输入图像尺寸
- 范围：640 ± 50%
- 优点：
  - 提高模型对不同尺度的适应能力
  - 增强模型泛化性

### 2. 自适应锚框 🎯

- 使用K-means聚类生成先验框
- 根据数据集特点自动调整
- 优点：
  - 提高检测精度
  - 加快收敛速度

### 3. Warmup and Cosine Learning Rate 📊

- Warmup：前几个epoch使用较小的学习率
- Cosine：学习率按余弦函数衰减
- 优点：
  - 稳定训练初期
  - 避免局部最优
  - 提高最终性能

### 4. 混合精度训练 ⚡

- 使用FP16和FP32混合精度
- 优点：
  - 减少显存占用
  - 加快训练速度
  - 保持精度

## 损失计算 📉

### 1. 边界框回归损失

- 使用CIoU Loss
- 考虑：
  - 重叠面积
  - 中心点距离
  - 长宽比
- 优点：
  - 更准确的边界框回归
  - 加快收敛速度

### 2. 置信度损失

- 使用BCE Loss
- 正样本权重更大
- 优点：
  - 平衡正负样本
  - 提高检测准确率

### 3. 分类损失 📊

- 使用BCE Loss
- 优点：
  - 多标签分类
  - 处理类别不平衡

### 4. 平衡不同尺度损失 ⚖️

- 对不同尺度的特征图使用不同权重
- 小目标权重更大
- 优点：
  - 提高小目标检测能力
  - 平衡不同尺度的性能

## 其他优化 🛠️

### 1. 消除Grid敏感度

- 使用sigmoid函数限制预测范围
- 优点：
  - 提高定位精度
  - 加快收敛速度

### 2. 匹配正样本

* **正样本** ：anchor 与 真实框 iou > 一定阈值,作为正样本
* **负样本** ：anchor 与 真实框 iou < 一定阈值,作为负样本, 负样本的置信度数设为0
* **忽略样本** ：某些预测框与真实框有一定重叠，但不足以作为正样本，也不应作为负样本（以避免混淆）。
* **Gridcell 分配**: 真实框中心落到gridcell中, 则该gridcell 的anchors 负责预测该目标

- 优点：
  - 提高召回率
  - 增强特征学习

# YOLOv8

## 网络结构

![]({{ site.baseurl }}/img/yolov8.png)

### backbone
#### stage
![]({{ site.baseurl }}/img/yolov8-stage.png)
1. Conv卷积层+Residual Block残差网络就被称为一个stage
2. 原始darknet-53中有一层卷积,图中红框, 在yolov8中去除了
   - 少做1024次卷积运算,少1024x3x3个参数
   - 参数减少后,过拟合现象不那么严重, 泛化能力增强

#### CBS
![]({{ site.baseurl }}/img/CBS.png)

一个二维卷积+二维度BN+SiLu激活函数

![]({{ site.baseurl }}/img/silu.png)
SiLU的激活是通过sigmoid函数乘以其输入来计算的，即xσ(x)。

优点:
  - 无上界(避免过拟合)
  - 有下界(产生更强的正则化效果)  
  - 平滑(处处可导,更容易训练)
  - x<0具有非单调性，函数值先降后增，-1以前是减函数，-1以后是增函数（对分布有重要意义 这点也是Swish和ReLU的最大区别）

正则化（Regularization）是机器学习和深度学习中防止模型过拟合的一种策略，它的核心思想是：
**在模型损失函数中加入一个"惩罚项"，限制模型复杂度，使模型在训练集上表现好同时又具备更好的泛化能力**

#### C2f
![]({{ site.baseurl }}/img/yolov8-backbone.png)

1. split:输入的特征图按照唯独分成两半
2. bottleneck之前的特征图和bottleneck之后的特征图进行拼接
3. n个bottleneck串行,每个bottleneck都和最后的一个bottleneck拼接起来, 相当于做了特征融合

优点: 让yolov8轻量化的同时获得更加丰富的梯度流信息 (有效的从输出到输入)

### neck

neck 部分的主要改进是用c2f代替c3, c2f的有点如上

### head

![]({{ site.baseurl }}/img/head对比.png)

1. 从原先的耦合头变成了解耦头
   - 一个head用来做目标识别, 损失函数包括Ciou 和 DFL
   - 一个head用来做分类, 损失函数用BCE
2. anchor-based 变成了anchor-free
  

### Anchor-Free机制解析

YOLOv8能够实现anchor-free主要基于以下几个关键设计：

#### 1. 直接预测中心点偏移

传统的anchor-based方法需要预测相对于anchor的偏移量，而YOLOv8直接预测目标框的中心点坐标：

$$
\begin{aligned}
x &= \sigma(t_x) \cdot stride + grid_x \\
y &= \sigma(t_y) \cdot stride + grid_y
\end{aligned}
$$

其中：
- $(t_x, t_y)$ 是网络预测的偏移量
- $\sigma$ 是sigmoid函数，将预测值限制在(0,1)范围内
- $stride$ 是特征图的步长
- $(grid_x, grid_y)$ 是网格单元的坐标

#### 2. 宽高预测方式

YOLOv8使用指数函数直接预测目标框的宽高：

$$
\begin{aligned}
w &= e^{t_w} \cdot stride \\
h &= e^{t_h} \cdot stride
\end{aligned}
$$

其中：
- $(t_w, t_h)$ 是网络预测的宽高缩放因子
- 指数函数确保宽高始终为正值
- 不再需要预设anchor的宽高比

#### 3. 优势

1. **简化设计**：
   - 无需手动设计anchor尺寸和比例
   - 减少超参数调优的工作量
   - 模型结构更加简洁

2. **更好的泛化性**：
   - 不受预设anchor的限制
   - 可以更好地适应不同尺度和形状的目标
   - 特别适合处理极端宽高比的目标

3. **训练更稳定**：
   - 直接预测绝对坐标，避免复杂的坐标变换
   - 减少训练过程中的数值不稳定性
   - 加快模型收敛速度

4. **计算效率更高**：
   - 减少anchor相关的计算开销
   - 降低内存占用
   - 提高推理速度

#### 4. 实现细节

1. **特征图设计**：
   - 使用多尺度特征图（P3, P4, P5）
   - 不同尺度的特征图负责检测不同大小的目标
   - 通过FPN和PAN结构增强特征融合

2. **预测头设计**：
   - 解耦的检测头设计
   - 分别预测中心点、宽高和类别
   - 使用DFL损失优化定位精度

3. **样本匹配策略**：
   - 使用Task-Aligned Assigner
   - 基于任务对齐分数进行正负样本分配
   - 动态调整匹配策略

## 损失计算

YOLOv8的损失函数由三部分组成：边界框回归损失、分类损失和DFL（Distribution Focal Loss）损失。

### 1. 边界框回归损失

使用CIoU Loss计算预测框与真实框之间的差异：

$$
L_{box} = 1 - IoU + \frac{\rho^2(b, b^{gt})}{c^2} + \alpha v
$$

其中：
- $IoU$ 是预测框与真实框的交并比
- $\rho^2(b, b^{gt})$ 是预测框中心点与真实框中心点的欧氏距离
- $c$ 是包含两个框的最小外接矩形的对角线长度
- $\alpha$ 是权重系数
- $v$ 是长宽比一致性项

### 2. 分类损失

使用二元交叉熵损失计算每个类别的预测概率：

$$
L_{cls} = -\sum_{c=1}^{C} [y_c \log(p_c) + (1-y_c)\log(1-p_c)]
$$

其中：
- $C$ 是类别总数
- $y_c$ 是真实标签（0或1）
- $p_c$ 是预测的类别概率

### 3. DFL损失

DFL（Distribution Focal Loss）是一种新的损失函数，用于处理边界框回归中的离散化问题：

$$
L_{DFL} = -\sum_{i=0}^{n} y_i \log(p_i)
$$

其中：
- $n$ 是离散化的区间数
- $y_i$ 是真实值在第i个区间的概率分布
- $p_i$ 是预测值在第i个区间的概率分布

### 总损失

最终的损失函数是三个部分的加权和：

$$
L_{total} = \lambda_1 L_{box} + \lambda_2 L_{cls} + \lambda_3 L_{DFL}
$$

其中 $\lambda_1$, $\lambda_2$, $\lambda_3$ 是各部分损失的权重系数。

## 样本匹配

> 抛弃anchor-base, 使用anchor-free
> 替代边长比例匹配,采用TaskAligned

### anchor-free

1. anchor-based
   - 利用anchor匹配正负样本, 缩小搜索空间,更准确,简单的进行梯度回传
   - 缺点: 训练匹配时开销较高；超参数多

2. anchor-free原理
   - 在特征图上的每一个像素视为候选框中心
   - 每个点预测: 分类分数, 框的位置(xywh), xy表示相对于网格左上角的偏移量
   - 训练目标是让预测框尽可能贴近GT框



### 正负样本分配
> yolov8 采用任务对齐匹配

🔁 匹配流程：

步骤 1️⃣：预选候选点
对每个 GT 框，在所有特征图层上找到所有候选点：
  - 这些点需要满足两个条件：
    - 落在 GT 框内部（或中心区域）；
    - 离 GT 框中心足够近（中心区域约束）；
  - 例如 FCOS 会用一个正方形区域作为候选区域。

步骤 2️⃣：计算每个候选点与 GT 框的 匹配分数（匹配质量）
匹配分数 = 分类得分 × IoU（或 L1 loss 的反函数）


步骤 3️⃣：筛选 Top-K 个点作为正样本
  - 对于每个 GT，只选择分数最高的几个点（如 10 个），作为正样本；
  - 其它点为负样本；


🎯 为什么要“任务对齐”？

**传统只看 IoU 选正样本，分类器和回归器优化目标不一致，容易训练不稳定；
任务对齐就像是：“找到同时分类和定位都不错的点”。**

